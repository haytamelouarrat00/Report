\chapter{Implementation, Result, and Analysis}
\section{Overview of the approach}
Before training, we have reviewed what the chosen network requires as input.
Since we are training using synthetic dataset that would imitate our target
scenarios, we focused on generating diverse and representative samples.
\section{Dataset Preparation \& Preprocessing}
The network expects:
\begin{itemize}
    \item \textbf{Two images:} the reference image, and the target image. Stored as numpy arrays.
    \item \textbf{Two sets of keypoints:}
          \begin{itemize}
              \item \textbf{Reference keypoints:} An array of keypoint coordinates found in the reference
                    image. It has a shape of $(N, 2)$, where N is the number of keypoints, and the coordinates are (x, y).
              \item \textbf{Target keypoints:} An array of keypoint coordinates found in the target image that correspond to the reference keypoints. It has the same shape $(N, 2)$.
          \end{itemize}
    \item \textbf{Matches:} An array of shape $(N, 2)$ that stores the indices of the matching keypoints. For example, $[i, j]$ means that the i-th keypoint in the reference keypoints matches the j-th keypoint in the target keypoints.
    \item \textbf{Homography Matrix: } A 3 by 3 matrix that relates the reference icon to the target one. It is used to transform points from the reference to the target image, and vice versa.
    \item \textbf{Two binary masks:} Binary masks of the target image that indicates the region where the pixel exists. This is useful to separate the foreground from the background.
\end{itemize}
These elements would be stored as a zipped numpy file (.npz) to be later accessed during training and evaluation, and stored in a folder structure that mirrors the dataset organization.
\subsection{Data Synthesis}
\subsubsection{General Approach}
To create a diverse and representative dataset, we employed data synthesis
techniques that will allow us to experiment with various scenarios. Our general
idea consists of cropping a region of interest from the selected gaming frames,
this will help us simulate different viewpoints where the targeted 2D template
may appear.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{ressources/roi.jpg}
    \caption{Example of a cropped region of interest from a gaming frame.}
    \label{fig:roi_example}
\end{figure}
This newly selected area will serve as our background for the rest of the synthesis process. Then, we select an icon from our set of templates, which will be our target during training. We overlay our icon onto the background at the center, and we ensure that the icon is fully contained within the background region by downscaling it slightly and continuously till it fits perfectly.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{ressources/image1.jpg}
    \caption{Example of the icon overlaid on the background.}
    \label{fig:overlay_example}
\end{figure}
While this describes our general design principle for synthetic data generation, the process was refined over several iterations to address specific challenges such as background complexity, icon transformations, and the robustness of extracted keypoints.
\begin{comment}
\begin{itemize}
    \item Scaling: We randomly scale the icon by a factor between 1.0 and 3.0
    \item Rotation: We randomly rotate the icon by an angle between -30 and 30 degrees.
    \item Translation: We randomly translate the icon by a few pixels in both x and y
          directions, while insuring that the icon remains within the bounds of the
          background.
    \item Color Jittering: We apply random changes to the brightness, contrast,
          saturation, and hue of the icon to create variations in appearance.
    \item Gaussian Noise: We add random Gaussian noise to the icon to simulate real-world
          noise.
\end{itemize}
\end{comment}

\section{Training Procedure \& Experimental Setup}
Our chosen approach consists of training the network on synthetic data using a
teacher-student framework. In the early stages, we kept the same teacher model
used by XFeat\cite{xfeat2023}. ALIKE, was used as a third party teacher tool to
extract ground truth keypoints from our training images.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ressources/image1_keypoints.jpg}
    \caption{Overview of extraction process.}
    \label{fig:teacher_student}
\end{figure}
We then generate a binary mask showing the presence of the icon within the background.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ressources/mask_1.png}
    \caption{Example of the binary mask generated for the icon.}
    \label{fig:binary_mask}
\end{figure}
This mask is used to filter out keypoints that do not belong to the icon, ensuring that only relevant features are considered during training. This would teach the model to focus only the keypoints and data existing within the icon region , which is what we want to achieve.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ressources/image1_keypoints_masked.jpg}
    \caption{Overview of filtering process.}
    \label{fig:teacher_student}
\end{figure}
By repeating this process on both \textbf{reference} and \textbf{target} images, we get a pair of keypoint arrays ready to match using ALIKE's matching algorithm to finally get our training data samples.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{ressources/example_data1.png}
    \caption{Example of the synthesis process, showing the background and the overlaid icon as well as the detected keypoints and masks.}
    \label{fig:synthesis_example1}
\end{figure}
At a first time we trained the model following the default parameters:
\begin{itemize}
    \item Learning rate: 0.001
    \item Batch size: 10
    \item Number of iterations: 160k
\end{itemize}
This has led us to under average results, prompting further investigation into hyperparameter tuning and data formulation.
\subsection{Results}
The metrics obtained from our initial training runs indicate that while the
model is capable of learning the basic features of the icons, it struggles with
more complex backgrounds and variations in icon appearance.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{ressources/loss_1.png}
    \caption{Loss curves for initial training runs.}
    \label{fig:loss_curves_1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{ressources/acc_1.png}
    \caption{Accuracy curves for initial training runs.}
    \label{fig:acc_curves_1}
\end{figure}
\subsubsection{Accuracy Metrics}
\begin{itemize}
    \item \textcolor{blue}{\textbf{Coarse Accuracy: }}Rapid increase in the first ~5k steps, reaching ~55\%-60\%. Then it plateaus with small oscillations.
    \item \textcolor{darkgreen}{\textbf{Keypoint Position Accuracy: }}Steady growth until ~20k steps, then saturates around ~25\%-27\%.
    \item \textcolor{myorange}{\textbf{Fine Matching Accuracy: }}Slow but consistent improvement, reaching ~20\%-22\% by step 50k, with more variance in later steps.
\end{itemize}
We can interpret this as follows:
\begin{itemize}
    \item The model learns coarse correspondences very quickly and maintains that
          performance, suggesting early layers or coarse matching blocks converge fast.
    \item Fine matching is the slowest to improve, likely because it depends on the
          coarse stage being strong first, and needs more subtle feature learning.
    \item Keypoint position accuracy plateaus mid-training, meaning spatial localization
          stops improving after ~20k steps.
\end{itemize}
\subsubsection{Loss Metrics}
\begin{itemize}
    \item \textcolor{blue}{\textbf{Descriptor Loss: }}Sharp drop in the first $\sim$5k steps, then remains low and stable (around 1--2).
    \item \textcolor{myorange}{\textbf{Fine Coordinates Loss: }}Slow but steady decrease across the 50k steps, still trending down by the end of training.
    \item \textcolor{darkgreen}{\textbf{Keypoint Position Loss: }}Drops quickly in early training and stabilizes, mirroring the accuracy plateau.
    \item \textcolor{red}{\textbf{Reliability Loss: }}Low from the start with minimal change, indicating stable confidence estimation.
    \item \textcolor{purple}{\textbf{Total Loss: }}Large drop in the first $\sim$5k steps, followed by a slower but continuous decline until the end.
\end{itemize}

We can interpret this as follows:
\begin{itemize}
    \item Most of the learning happens in the first 5k--10k steps, especially for
          descriptors and keypoint positioning.
    \item Fine coordinate refinement remains the main bottleneck, as its loss is still
          higher and accuracy is still improving, suggesting more training could help.
    \item Reliability loss stability shows that confidence estimation remains consistent
          throughout training.
\end{itemize}

\subsubsection{Confusion matrix}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ressources/confusion_matrix_1.png}
    \caption{Confusion matrix for initial training runs.}
    \label{fig:confusion_matrix_1}
\end{figure}
The confusion matrix shows the following values:
\begin{itemize}
    \item \textbf{True Positives (TP):} 124,735 cases where positive samples were correctly classified.
    \item \textbf{False Negatives (FN):} 265 cases where positive samples were incorrectly classified as negative.
    \item \textbf{False Positives (FP):} 306 cases where negative samples were incorrectly classified as positive.
    \item \textbf{True Negatives (TN):} 124,694 cases where negative samples were correctly classified.
\end{itemize}

From these values, we can compute:
\begin{itemize}
    \item \textbf{Accuracy:} $\frac{TP + TN}{TP + TN + FP + FN} \approx 99.77\%$
    \item \textbf{Precision:} $\frac{TP}{TP + FP} \approx 99.76\%$
    \item \textbf{Recall:} $\frac{TP}{TP + FN} \approx 99.79\%$
    \item \textbf{F1-Score:} $\approx 99.78\%$
\end{itemize}

These results indicate that the model performs extremely well, with both
precision and recall above 99.7\%, suggesting minimal misclassification in both
classes, but not enough, as False positives are extremely important in our
case, as they would lead to false matches, and ultimately impact the user
experience negatively.
\subsubsection{Overall Assessment}
The training shows strong performance in terms of fast convergence on coarse
matching and descriptor learning, with stable behavior and no signs of
divergence. However, fine matching accuracy remains low compared to coarse
accuracy, and keypoint accuracy saturates early, indicating possible
underfitting in fine localization. \textbf{To address these issues, several
    strategies can be considered: extending the training or applying fine-tuning
    with a smaller learning rate to further improve fine accuracy, reweighting the
    losses to give more emphasis to the FineCoords component once the coarse stage
    has converged, and applying data augmentation techniques that highlight
    sub-pixel differences to strengthen fine matching capabilities. Additionally, a
    curriculum learning approach could be adopted, starting with simpler examples
    before progressively increasing difficulty for the fine stage. }

\section{Iterative Refinement}
During experimentation, we implemented and evaluated three alternative
synthesis pipelines derived from our general concept:
\begin{enumerate}
    \item \textbf{NAFM A – Static Background-Icon, Transformed Keypoints:}
          The background and icons remained unchanged, while the keypoints were transformed using the homography matrix generated during the synthesis process. This matrix was applied to map the keypoints from the reference image to the target image, rather than re-extracting them from both images using the teacher model.
    \item \textbf{NAFM B – Cropped Background/Dynamic Icon-Transformed Keypoints:}
          The reference image was cropped down to focus on the area of interest, and ignore the background. The target icon was transformed using translation, placing it in a random position within the background.
    \item \textbf{NAFM C – Unfiltered SIFT Features}
          The reference and target images were not changed from the variant B, but the keypoints were extracted and matched using SIFT, without filtering them using the binary mask.
\end{enumerate}
Each variant maintained the core pipeline (ROI selection, overlay, keypoint extraction, matching), but emphasized different aspects of visual variability.
\subsection{Variant A}
In variant A, we focused on maintaining a static background and icon, while
transforming the keypoints obtained from the reference image to the target
image using a homography matrix. This would allow us to generate high quality
matches, as our objective is to find absolute correspondences between the
reference and the query templates. So we thought that using the transformed
keypoints would yield better results than re-extracting them from the target
image.
\subsubsection{Homography-based keypoint transfer}

Let the reference keypoints be $\mathcal{X}=\{ \mathbf{x}_i \}_{i=1}^{N}$ with
$\mathbf{x}_i=(u_i,v_i)^\top$ in pixel coordinates. Write them in homogeneous
form $\tilde{\mathbf{x}}_i=(u_i,v_i,1)^\top$. Let the homography be
$H\in\mathbb{R}^{3\times 3}$.

The mapped keypoints on the target image are
\[
    \tilde{\mathbf{x}}'_i \sim H\,\tilde{\mathbf{x}}_i,
    \qquad
    \mathbf{x}'_i=\Big(\frac{\tilde{u}'_i}{\tilde{w}'_i},\,
    \frac{\tilde{v}'_i}{\tilde{w}'_i}\Big)^\top,
\]
where $\tilde{\mathbf{x}}'_i=(\tilde{u}'_i,\tilde{v}'_i,\tilde{w}'_i)^\top$. In
matrix form for all points:
\[
    \tilde{X}' \sim H \tilde{X}, \quad
    X'=\begin{bmatrix}
        \tilde{X}'_{1,:}/\tilde{X}'_{3,:} \\[2pt]
        \tilde{X}'_{2,:}/\tilde{X}'_{3,:}
    \end{bmatrix}^{\!\top}.
\]

\paragraph{Notes.}
If $\tilde{w}'_i=0$ the point maps to infinity and is discarded.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ressources/image2.png}
    \caption{Example of the synthesis process for variant A}
    \label{fig:variant_a}
\end{figure}

Although this approach seems promising, it relies heavily on the quality of our
data synthesis process, and the homography matrix generation. In practice, we
found that the performance was sensitive to these factors.
\subsubsection{Accuracy metrics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ressources/acuracy_2.png}
    \caption{Example of the accuracy metrics evaluation}
    \label{fig:accuracy_metrics_2}
\end{figure}
\begin{itemize}
    \item \textcolor{blue}{\textbf{Coarse Accuracy: }}Starts low (~55\%) and quickly stabilizes around 60\%. Stable without large oscillations — suggests the coarse-level model component converged early.
    \item \textcolor{myorange}{\textbf{Fine Accuracy: }}Gradual increase from ~20\% to ~47\% over training — indicates steady improvement in fine-grained matching.
    \item \textcolor{darkgreen}{\textbf{Keypoint Position: }}Rises from ~10\% to ~36\%, slower than coarse accuracy, but still trending upward — likely due to higher difficulty of precise keypoint localization.
\end{itemize}
\subsubsection{Loss metrics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ressources/multiple_losses_graph_2.png}
    \caption{Example of the loss metrics evaluation}
    \label{fig:loss_metrics_2}
\end{figure}
\begin{itemize}
    \item \textcolor{blue}{\textbf{Descriptor Loss:}} Rapid drop from ~17 to ~1.5 in the first 5k–10k steps, then slow decline — feature descriptors are learning quickly.
    \item \textcolor{myorange}{\textbf{Fine Coords Loss:}} Drops from ~8 to ~2.2, showing better coordinate regression.
    \item \textcolor{darkgreen}{\textbf{Keypoint Position Loss:}} Drops from ~4.5 to ~2.2 — still higher than fine coords, implying room for improvement in keypoint position accuracy.
    \item \textcolor{purple}{\textbf{Reliability Loss:}} Drops from ~2.5 to ~1.0 — confidence estimation is improving.
    \item \textcolor{red}{\textbf{Total Loss:}} Drops sharply from ~45 to ~12–14, then plateaus — convergence reached.
\end{itemize}
\subsubsection{Confusion Matrix}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ressources/confusion_matrix_2.png}
    \caption{Confusion matrix for variant A}
    \label{fig:confusion_matrix_2}
\end{figure}
\begin{itemize}
    \item \textbf{Accuracy :} 0.602
    \item \textbf{Precision :} 0.696
    \item \textbf{Recall :} 0.363
    \item \textbf{F1 Score :} 0.477
\end{itemize}
\subsubsection{Interpretation}
The results show that while the model can differentiate between various
features and capture some general patterns in the data, it struggles to truly
learn our custom target. The main problems are the rapid overfitting seen in
the loss curves and the overall low accuracy. To mitigate this, we introduced
learning rate scheduling and applied a basic form of data augmentation:
cropping the background from the reference image. This was done to prevent the
network from memorizing irrelevant background details instead of focusing on
the icon itself. Another issue was that the loss metrics operated on very
different scales, which could skew the optimization process. To address this,
we adjusted their relative weights so they would be more balanced. In
particular, we lowered the weight of the keypoint position loss, as it was
consistently the largest, to avoid it dominating the training signal. Despite
these changes, the network still fails to capture the distinctive patterns and
features that define our custom target. This suggests that the model is
focusing on general cues or noise rather than learning the precise visual
characteristics we aim to detect.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ressources/false_positive_1.png}
    \caption{False Positive Example}
    \label{fig:false_positive_1}
\end{figure}

\subsection{Variant B}
In our new solution, we noticed that some icons contained semi-transparent
regions. When placed over a background, these pixels blended with the
background colors, creating a faint “merged” zone around and inside the icon.
This blending made the icon's edges less distinct and introduced background
information into what should be pure icon pixels. As a result, the network
could extract features from these shaded areas that are not native to the icon,
leading to confusing and noisy data and making it harder to learn accurate
features. To address this, we added a step in the data generation pipeline that
removes any pixel whose alpha value is below a chosen threshold, keeping only
fully opaque parts of the icon and ensuring a clean separation from the
background.
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/icon_4_original.png}
        \caption{Original Icon}
        \label{fig:original_icon}
    \end{minipage}
    \hfill
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/icon_4.png}
        \caption{Processed Icon}
        \label{fig:processed_icon}
    \end{minipage}
\end{figure}
Next we have added simple geometric transformations to our synthesis pipeline, where we would move the icon by random translations, to imitate the icon's position variability in real-world scenarios. And then we would keep the rest of the pipeline intact.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ressources/icon_transformed.png}
    \caption{Transformed Icon}
    \label{fig:transformed_icon}
\end{figure}
\subsubsection{Accuracy metrics}
\subsubsection{Loss metrics}
\subsubsection{Confusion matrix}
\subsubsection{Interpretation}

\subsection{Variant C}
We decided to return to our original approach and adopt SIFT as the teacher method. Given that our SIFT-based solution already demonstrates strong performance on the target task, it provides a robust and reliable supervisory signal for training. In addition, we deliberately removed all forms of filtering, such as RANSAC and masking, in order to increase the complexity of the learning problem and reduce the risk of overfitting. This choice results in a more raw and unfiltered dataset, encouraging the student model to generalize more effectively under challenging conditions.
\section{Analysis \& Discussion}
\subsection{Compare results with SIFT, ORB, XFeat}
\subsection{Discuss trade-offs: speed vs. accuracy.}
\subsection{Link back to performance targets from Chapter 4.}
\subsection{Tie to the research gap from Chapter 3.}

